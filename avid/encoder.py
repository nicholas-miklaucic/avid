"""Encoder/decoder for crystal data."""

from typing import Callable
from einops import rearrange
import jax
import jax.numpy as jnp
from jaxtyping import Float, Array, Int, Bool
from flax import linen as nn

from avid.config import MainConfig
from avid.dataset import DataBatch, load_file
from avid.utils import ELEM_VALS, debug_structure, flax_summary, tcheck
from avid.layers import Identity, LazyInOutMLP


class SpeciesEmbed(nn.Module):
    """Species density embedding."""

    n_species: int
    species_embed_dim: int
    dim_out: int
    embed_module: LazyInOutMLP

    @tcheck
    @nn.compact
    def __call__(
        self,
        x: Float[Array, ''],
        spec: Int[Array, ''],
        mask: Bool[Array, ''],
        training: bool,
    ) -> Float[Array, '_dim_out']:
        spec_embed = nn.Embed(self.n_species, self.species_embed_dim, name='species_embed')

        input_embeds = jnp.concat([spec_embed(spec), x[..., None]], axis=-1)
        return self.embed_module(input_embeds, self.dim_out, training) * mask.astype(jnp.float32)


class ReduceSpeciesEmbed(nn.Module):
    species_embed: SpeciesEmbed

    @tcheck
    @nn.compact
    def __call__(self, data: DataBatch, training: bool) -> Float[Array, 'batch nx ny nz _dim_out']:
        spec1 = jax.vmap(
            lambda x, spec, mask: self.species_embed(x=x, spec=spec, mask=mask, training=training)
        )

        spec2 = jax.vmap(spec1, in_axes=-1, out_axes=-1)

        x = jax.lax.collapse(data.density, 1, 4)
        nx, ny, nz = data.density.shape[1:4]
        debug_structure(x=x, spec=data.species, mask=data.mask)
        return rearrange(
            jax.vmap(spec2, in_axes=(1, None, None), out_axes=1)(x, data.species, data.mask).sum(
                axis=-1
            ),
            'b (nx ny nz) c -> b nx ny nz c',
            nx=nx,
            ny=ny,
            nz=nz,
        )


class Downsample(nn.Module):
    downsample_factor: int
    channel_out: int
    kernel_size: int
    kernel_init: Callable = nn.initializers.truncated_normal(stddev=0.1)
    bias_init: Callable = nn.initializers.truncated_normal()

    def setup(self):
        if (self.kernel_size - 1) // 2 < (self.downsample_factor - 1):
            raise ValueError(f'Configuration {self} would skip data!')

        self.conv = nn.Conv(
            features=self.channel_out,
            kernel_size=[self.kernel_size for _ in range(3)],
            strides=[self.downsample_factor for _ in range(3)],
            padding='CIRCULAR',
            name='conv',
            kernel_init=self.kernel_init,
            bias_init=self.bias_init,
        )

    @tcheck
    def __call__(
        self, x: Float[Array, '*batch D H W C']
    ) -> Float[Array, '*batch _d_less _h_less _w_less _c_out']:
        return self.conv(x)


if __name__ == '__main__':
    config = MainConfig()
    embed_mlp = LazyInOutMLP(
        inner_dims=(32,),
    )
    spec_embed = SpeciesEmbed(
        len(ELEM_VALS), species_embed_dim=16, dim_out=8, embed_module=embed_mlp
    )
    downsample = Downsample(downsample_factor=2, channel_out=10, kernel_size=3)
    key = jax.random.key(12345)

    batch = load_file(config, 0)

    kwargs = dict(training=True)

    mod = nn.Sequential([ReduceSpeciesEmbed(spec_embed), downsample])
    out, params = mod.init_with_output(key, data=batch, training=True)

    debug_structure(batch=batch, module=mod, out=out)
    flax_summary(mod, batch, **kwargs)

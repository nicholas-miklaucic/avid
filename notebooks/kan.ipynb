{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import rho_plus as rp\n",
    "\n",
    "from rich.pretty import pprint\n",
    "\n",
    "is_dark = False\n",
    "theme, cs = rp.mpl_setup(is_dark)\n",
    "rp.plotly_setup(is_dark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nmiklaucic/avid\n"
     ]
    }
   ],
   "source": [
    "%cd ~/avid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5637 35800\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0-norm</th>\n",
       "      <th>2-norm</th>\n",
       "      <th>3-norm</th>\n",
       "      <th>5-norm</th>\n",
       "      <th>7-norm</th>\n",
       "      <th>10-norm</th>\n",
       "      <th>minimum Number</th>\n",
       "      <th>maximum Number</th>\n",
       "      <th>range Number</th>\n",
       "      <th>mean Number</th>\n",
       "      <th>...</th>\n",
       "      <th>avg s valence electrons</th>\n",
       "      <th>avg p valence electrons</th>\n",
       "      <th>avg d valence electrons</th>\n",
       "      <th>avg f valence electrons</th>\n",
       "      <th>compound possible</th>\n",
       "      <th>max ionic char</th>\n",
       "      <th>avg ionic char</th>\n",
       "      <th>magmom_pa</th>\n",
       "      <th>bandgap</th>\n",
       "      <th>delta_e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.544139</td>\n",
       "      <td>4.304959</td>\n",
       "      <td>4.099305</td>\n",
       "      <td>3.846319</td>\n",
       "      <td>3.738150</td>\n",
       "      <td>3.663704</td>\n",
       "      <td>2.864847</td>\n",
       "      <td>0.010989</td>\n",
       "      <td>-1.760193</td>\n",
       "      <td>1.580818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.737291</td>\n",
       "      <td>-0.899290</td>\n",
       "      <td>2.895815</td>\n",
       "      <td>-0.492369</td>\n",
       "      <td>1.125155</td>\n",
       "      <td>-1.720739</td>\n",
       "      <td>-1.587831</td>\n",
       "      <td>-0.353321</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.003319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.544139</td>\n",
       "      <td>4.304959</td>\n",
       "      <td>4.099305</td>\n",
       "      <td>3.846319</td>\n",
       "      <td>3.738150</td>\n",
       "      <td>3.663704</td>\n",
       "      <td>-0.670320</td>\n",
       "      <td>-2.027018</td>\n",
       "      <td>-1.760193</td>\n",
       "      <td>-1.302375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.737291</td>\n",
       "      <td>-1.778907</td>\n",
       "      <td>-0.937909</td>\n",
       "      <td>-0.492369</td>\n",
       "      <td>1.125155</td>\n",
       "      <td>-1.720739</td>\n",
       "      <td>-1.587831</td>\n",
       "      <td>-0.353340</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.108143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.544139</td>\n",
       "      <td>4.304959</td>\n",
       "      <td>4.099305</td>\n",
       "      <td>3.846319</td>\n",
       "      <td>3.738150</td>\n",
       "      <td>3.663704</td>\n",
       "      <td>4.671710</td>\n",
       "      <td>1.052637</td>\n",
       "      <td>-1.760193</td>\n",
       "      <td>3.054449</td>\n",
       "      <td>...</td>\n",
       "      <td>0.737291</td>\n",
       "      <td>-1.778907</td>\n",
       "      <td>-0.171164</td>\n",
       "      <td>5.574266</td>\n",
       "      <td>1.125155</td>\n",
       "      <td>-1.720739</td>\n",
       "      <td>-1.587831</td>\n",
       "      <td>-0.353355</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.071216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.544139</td>\n",
       "      <td>4.304959</td>\n",
       "      <td>4.099305</td>\n",
       "      <td>3.846319</td>\n",
       "      <td>3.738150</td>\n",
       "      <td>3.663704</td>\n",
       "      <td>0.193832</td>\n",
       "      <td>-1.528839</td>\n",
       "      <td>-1.760193</td>\n",
       "      <td>-0.597594</td>\n",
       "      <td>...</td>\n",
       "      <td>0.737291</td>\n",
       "      <td>0.859944</td>\n",
       "      <td>-0.937909</td>\n",
       "      <td>-0.492369</td>\n",
       "      <td>1.125155</td>\n",
       "      <td>-1.720739</td>\n",
       "      <td>-1.587831</td>\n",
       "      <td>1.805382</td>\n",
       "      <td>2.0113</td>\n",
       "      <td>3.509988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-2.544139</td>\n",
       "      <td>4.304959</td>\n",
       "      <td>4.099305</td>\n",
       "      <td>3.846319</td>\n",
       "      <td>3.738150</td>\n",
       "      <td>3.663704</td>\n",
       "      <td>0.743747</td>\n",
       "      <td>-1.211815</td>\n",
       "      <td>-1.760193</td>\n",
       "      <td>-0.149098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.737291</td>\n",
       "      <td>-1.778907</td>\n",
       "      <td>-0.171164</td>\n",
       "      <td>-0.492369</td>\n",
       "      <td>1.125155</td>\n",
       "      <td>-1.720739</td>\n",
       "      <td>-1.587831</td>\n",
       "      <td>-0.353329</td>\n",
       "      <td>0.1069</td>\n",
       "      <td>0.114489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84182</th>\n",
       "      <td>-0.302404</td>\n",
       "      <td>1.073151</td>\n",
       "      <td>1.155693</td>\n",
       "      <td>1.189103</td>\n",
       "      <td>1.189594</td>\n",
       "      <td>1.184777</td>\n",
       "      <td>-0.277523</td>\n",
       "      <td>-0.441901</td>\n",
       "      <td>-0.302464</td>\n",
       "      <td>-0.524370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416977</td>\n",
       "      <td>1.362582</td>\n",
       "      <td>-0.855758</td>\n",
       "      <td>-0.492369</td>\n",
       "      <td>1.125155</td>\n",
       "      <td>1.435748</td>\n",
       "      <td>1.399402</td>\n",
       "      <td>-0.353365</td>\n",
       "      <td>7.1220</td>\n",
       "      <td>-4.235145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84184</th>\n",
       "      <td>-0.302404</td>\n",
       "      <td>-0.229039</td>\n",
       "      <td>-0.384105</td>\n",
       "      <td>-0.537700</td>\n",
       "      <td>-0.599189</td>\n",
       "      <td>-0.629952</td>\n",
       "      <td>-0.748879</td>\n",
       "      <td>1.143215</td>\n",
       "      <td>1.689766</td>\n",
       "      <td>-0.751365</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.056469</td>\n",
       "      <td>-0.019673</td>\n",
       "      <td>-0.784560</td>\n",
       "      <td>0.114294</td>\n",
       "      <td>1.125155</td>\n",
       "      <td>0.961428</td>\n",
       "      <td>1.501256</td>\n",
       "      <td>-0.353302</td>\n",
       "      <td>3.6820</td>\n",
       "      <td>-2.416089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84186</th>\n",
       "      <td>-0.302404</td>\n",
       "      <td>-0.032556</td>\n",
       "      <td>0.033257</td>\n",
       "      <td>0.130844</td>\n",
       "      <td>0.170592</td>\n",
       "      <td>0.193219</td>\n",
       "      <td>-0.356083</td>\n",
       "      <td>1.052637</td>\n",
       "      <td>1.349629</td>\n",
       "      <td>0.158443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.737291</td>\n",
       "      <td>0.332174</td>\n",
       "      <td>-0.784560</td>\n",
       "      <td>0.720958</td>\n",
       "      <td>1.125155</td>\n",
       "      <td>0.989041</td>\n",
       "      <td>1.397058</td>\n",
       "      <td>-0.353266</td>\n",
       "      <td>3.8500</td>\n",
       "      <td>-3.617353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84187</th>\n",
       "      <td>-0.302404</td>\n",
       "      <td>-0.032556</td>\n",
       "      <td>0.033257</td>\n",
       "      <td>0.130844</td>\n",
       "      <td>0.170592</td>\n",
       "      <td>0.193219</td>\n",
       "      <td>-0.356083</td>\n",
       "      <td>1.097926</td>\n",
       "      <td>1.398220</td>\n",
       "      <td>0.158443</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.159589</td>\n",
       "      <td>0.332174</td>\n",
       "      <td>-0.707885</td>\n",
       "      <td>0.720958</td>\n",
       "      <td>1.125155</td>\n",
       "      <td>1.100757</td>\n",
       "      <td>1.370412</td>\n",
       "      <td>-0.080313</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.118394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84189</th>\n",
       "      <td>-0.302404</td>\n",
       "      <td>-0.032556</td>\n",
       "      <td>0.033257</td>\n",
       "      <td>0.130844</td>\n",
       "      <td>0.170592</td>\n",
       "      <td>0.193219</td>\n",
       "      <td>-0.513201</td>\n",
       "      <td>-1.075948</td>\n",
       "      <td>-0.836965</td>\n",
       "      <td>-0.853878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.737291</td>\n",
       "      <td>0.684021</td>\n",
       "      <td>-0.554536</td>\n",
       "      <td>-0.492369</td>\n",
       "      <td>1.125155</td>\n",
       "      <td>0.310794</td>\n",
       "      <td>0.114810</td>\n",
       "      <td>0.366224</td>\n",
       "      <td>3.6240</td>\n",
       "      <td>-1.933853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41437 rows × 148 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0-norm    2-norm    3-norm    5-norm    7-norm   10-norm  \\\n",
       "0     -2.544139  4.304959  4.099305  3.846319  3.738150  3.663704   \n",
       "2     -2.544139  4.304959  4.099305  3.846319  3.738150  3.663704   \n",
       "3     -2.544139  4.304959  4.099305  3.846319  3.738150  3.663704   \n",
       "4     -2.544139  4.304959  4.099305  3.846319  3.738150  3.663704   \n",
       "10    -2.544139  4.304959  4.099305  3.846319  3.738150  3.663704   \n",
       "...         ...       ...       ...       ...       ...       ...   \n",
       "84182 -0.302404  1.073151  1.155693  1.189103  1.189594  1.184777   \n",
       "84184 -0.302404 -0.229039 -0.384105 -0.537700 -0.599189 -0.629952   \n",
       "84186 -0.302404 -0.032556  0.033257  0.130844  0.170592  0.193219   \n",
       "84187 -0.302404 -0.032556  0.033257  0.130844  0.170592  0.193219   \n",
       "84189 -0.302404 -0.032556  0.033257  0.130844  0.170592  0.193219   \n",
       "\n",
       "       minimum Number  maximum Number  range Number  mean Number  ...  \\\n",
       "0            2.864847        0.010989     -1.760193     1.580818  ...   \n",
       "2           -0.670320       -2.027018     -1.760193    -1.302375  ...   \n",
       "3            4.671710        1.052637     -1.760193     3.054449  ...   \n",
       "4            0.193832       -1.528839     -1.760193    -0.597594  ...   \n",
       "10           0.743747       -1.211815     -1.760193    -0.149098  ...   \n",
       "...               ...             ...           ...          ...  ...   \n",
       "84182       -0.277523       -0.441901     -0.302464    -0.524370  ...   \n",
       "84184       -0.748879        1.143215      1.689766    -0.751365  ...   \n",
       "84186       -0.356083        1.052637      1.349629     0.158443  ...   \n",
       "84187       -0.356083        1.097926      1.398220     0.158443  ...   \n",
       "84189       -0.513201       -1.075948     -0.836965    -0.853878  ...   \n",
       "\n",
       "       avg s valence electrons  avg p valence electrons  \\\n",
       "0                     0.737291                -0.899290   \n",
       "2                     0.737291                -1.778907   \n",
       "3                     0.737291                -1.778907   \n",
       "4                     0.737291                 0.859944   \n",
       "10                    0.737291                -1.778907   \n",
       "...                        ...                      ...   \n",
       "84182                 0.416977                 1.362582   \n",
       "84184                -1.056469                -0.019673   \n",
       "84186                 0.737291                 0.332174   \n",
       "84187                -0.159589                 0.332174   \n",
       "84189                 0.737291                 0.684021   \n",
       "\n",
       "       avg d valence electrons  avg f valence electrons  compound possible  \\\n",
       "0                     2.895815                -0.492369           1.125155   \n",
       "2                    -0.937909                -0.492369           1.125155   \n",
       "3                    -0.171164                 5.574266           1.125155   \n",
       "4                    -0.937909                -0.492369           1.125155   \n",
       "10                   -0.171164                -0.492369           1.125155   \n",
       "...                        ...                      ...                ...   \n",
       "84182                -0.855758                -0.492369           1.125155   \n",
       "84184                -0.784560                 0.114294           1.125155   \n",
       "84186                -0.784560                 0.720958           1.125155   \n",
       "84187                -0.707885                 0.720958           1.125155   \n",
       "84189                -0.554536                -0.492369           1.125155   \n",
       "\n",
       "       max ionic char  avg ionic char  magmom_pa  bandgap   delta_e  \n",
       "0           -1.720739       -1.587831  -0.353321   0.0000  0.003319  \n",
       "2           -1.720739       -1.587831  -0.353340   0.0000  0.108143  \n",
       "3           -1.720739       -1.587831  -0.353355   0.0000  0.071216  \n",
       "4           -1.720739       -1.587831   1.805382   2.0113  3.509988  \n",
       "10          -1.720739       -1.587831  -0.353329   0.1069  0.114489  \n",
       "...               ...             ...        ...      ...       ...  \n",
       "84182        1.435748        1.399402  -0.353365   7.1220 -4.235145  \n",
       "84184        0.961428        1.501256  -0.353302   3.6820 -2.416089  \n",
       "84186        0.989041        1.397058  -0.353266   3.8500 -3.617353  \n",
       "84187        1.100757        1.370412  -0.080313   0.0000  1.118394  \n",
       "84189        0.310794        0.114810   0.366224   3.6240 -1.933853  \n",
       "\n",
       "[41437 rows x 148 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_splits = (1,2,3,4,5,6,7)\n",
    "batch_size = 512\n",
    "\n",
    "df = pd.read_feather('data/mpc_full_feats_scaled_split.feather')\n",
    "df = df[df['dataset_split'].isin(dataset_splits)]\n",
    "is_valid = df['Xshift_umap']\n",
    "print(np.sum(is_valid), np.sum(~is_valid))\n",
    "df = df.select_dtypes('number').drop(columns=['TSNE_x', 'TSNE_y', 'umap_x', 'umap_y', 'dataset_split'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[3m                                  KAN Summary                                   \u001b[0m\n",
      "┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mpath     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams        \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n",
      "│           │ KAN            │ \u001b[2mfloat32\u001b[0m[14,4] │ \u001b[2mfloat32\u001b[0m[14,6]  │                │\n",
      "├───────────┼────────────────┼───────────────┼────────────────┼────────────────┤\n",
      "│ norms_0   │ LayerNorm      │ \u001b[2mfloat32\u001b[0m[4]    │ \u001b[2mfloat32\u001b[0m[4]     │ bias:          │\n",
      "│           │                │               │                │ \u001b[2mfloat32\u001b[0m[4]     │\n",
      "│           │                │               │                │ scale:         │\n",
      "│           │                │               │                │ \u001b[2mfloat32\u001b[0m[4]     │\n",
      "│           │                │               │                │                │\n",
      "│           │                │               │                │ \u001b[1m8 \u001b[0m\u001b[1;2m(32 B)\u001b[0m       │\n",
      "├───────────┼────────────────┼───────────────┼────────────────┼────────────────┤\n",
      "│ layers_0  │ KANLayer       │ \u001b[2mfloat32\u001b[0m[4]    │ postacts:      │ coef:          │\n",
      "│           │ \u001b[2m(full_output)\u001b[0m  │               │ \u001b[2mfloat32\u001b[0m[5,4]   │ \u001b[2mfloat32\u001b[0m[20,8]  │\n",
      "│           │                │               │ postspline:    │                │\n",
      "│           │                │               │ \u001b[2mfloat32\u001b[0m[5,4]   │ \u001b[1m160 \u001b[0m\u001b[1;2m(640 B)\u001b[0m    │\n",
      "│           │                │               │ y: \u001b[2mfloat32\u001b[0m[5]  │                │\n",
      "├───────────┼────────────────┼───────────────┼────────────────┼────────────────┤\n",
      "│ norms_1   │ LayerNorm      │ \u001b[2mfloat32\u001b[0m[5]    │ \u001b[2mfloat32\u001b[0m[5]     │ bias:          │\n",
      "│           │                │               │                │ \u001b[2mfloat32\u001b[0m[5]     │\n",
      "│           │                │               │                │ scale:         │\n",
      "│           │                │               │                │ \u001b[2mfloat32\u001b[0m[5]     │\n",
      "│           │                │               │                │                │\n",
      "│           │                │               │                │ \u001b[1m10 \u001b[0m\u001b[1;2m(40 B)\u001b[0m      │\n",
      "├───────────┼────────────────┼───────────────┼────────────────┼────────────────┤\n",
      "│ layers_1  │ KANLayer       │ \u001b[2mfloat32\u001b[0m[5]    │ postacts:      │ coef:          │\n",
      "│           │ \u001b[2m(full_output)\u001b[0m  │               │ \u001b[2mfloat32\u001b[0m[6,5]   │ \u001b[2mfloat32\u001b[0m[30,8]  │\n",
      "│           │                │               │ postspline:    │                │\n",
      "│           │                │               │ \u001b[2mfloat32\u001b[0m[6,5]   │ \u001b[1m240 \u001b[0m\u001b[1;2m(960 B)\u001b[0m    │\n",
      "│           │                │               │ y: \u001b[2mfloat32\u001b[0m[6]  │                │\n",
      "├───────────┼────────────────┼───────────────┼────────────────┼────────────────┤\n",
      "│ final_act │ Identity       │ \u001b[2mfloat32\u001b[0m[6]    │ \u001b[2mfloat32\u001b[0m[6]     │                │\n",
      "├───────────┼────────────────┼───────────────┼────────────────┼────────────────┤\n",
      "│\u001b[1m \u001b[0m\u001b[1m         \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m              \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m             \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m         Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m418 \u001b[0m\u001b[1;2m(1.7 KB)\u001b[0m\u001b[1m  \u001b[0m\u001b[1m \u001b[0m│\n",
      "└───────────┴────────────────┴───────────────┴────────────────┴────────────────┘\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[1m                         Total Parameters: 418 \u001b[0m\u001b[1;2m(1.7 KB)\u001b[0m\u001b[1m                         \u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable, Sequence\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import flax.linen as nn\n",
    "from flax import struct\n",
    "from jaxtyping import Float, Array\n",
    "from eins import EinsOp\n",
    "import functools as ft\n",
    "\n",
    "from avid.layers import Identity\n",
    "from avid.utils import debug_structure, flax_summary\n",
    "\n",
    "jax.config.update('jax_debug_nans', False)\n",
    "\n",
    "eps = 1e-12\n",
    "\n",
    "@ft.partial(jax.jit, static_argnames=('k', 'extend'))\n",
    "def eval_spline(x: Float[Array, \"splines\"], grid: Float[Array, \"splines grid\"], k: int = 0, extend: bool = True) -> Float[Array, \"splines coefs=grid+k-1\"]:\n",
    "    \"\"\"Evaluate x on B-spline bases.\"\"\"\n",
    "    if extend:\n",
    "        h = (grid[:, [-1]] - grid[:, [0]]) / (grid.shape[1] - 1)\n",
    "        pad_start = jnp.tile(grid[:, [0]], (1, k)) - h\n",
    "        pad_end = jnp.tile(grid[:, [-1]], (1, k)) + h\n",
    "        grid = jnp.concat([pad_start, grid, pad_end], axis=1)    \n",
    "\n",
    "    if x.ndim == 1:\n",
    "        x = x[..., None]\n",
    "        \n",
    "    # debug_structure(x=x, grid=grid)\n",
    "\n",
    "    if k == 0:\n",
    "        value = ((x >= grid[:, :-1]) * (x < grid[:, 1:])).astype(x.dtype)\n",
    "    else:        \n",
    "        B_km1 = eval_spline(x, grid=grid, k=k - 1, extend=False)        \n",
    "        value = (x - grid[:, :-(k + 1)])        \n",
    "        value = value / (grid[:, k:-1] - grid[:, :-(k + 1)] + 1e-12)        \n",
    "        value = value * B_km1[:, :-1]\n",
    "        value = value + (grid[:, k + 1:] - x) / (grid[:, k + 1:] - grid[:, 1:(-k)] + eps) * B_km1[:, 1:]\n",
    "    return value\n",
    "\n",
    "@ft.partial(jax.jit, static_argnames='k')\n",
    "def coef2curve(x_eval: Float[Array, \"splines\"], grid: Float[Array, \"splines grid\"], coef: Float[Array, \"splines coefs\"], k: int) -> Float[Array, \"splines\"]:\n",
    "    \"\"\"converting B-spline coefficients to B-spline curves. Evaluate x on B-spline curves (summing\n",
    "    up B_batch results over B-spline basis).\"\"\"    \n",
    "    return jnp.einsum('sc,sc->s', coef, eval_spline(x_eval, grid, k))    \n",
    "\n",
    "\n",
    "@ft.partial(jax.jit, static_argnames='k')\n",
    "def curve2coef(x_eval: Float[Array, \"samples splines\"], y_eval: Float[Array, \"samples splines\"], grid: Float[Array, \"splines grid\"], k: int) -> Float[Array, \"splines coefs\"]:\n",
    "    '''\n",
    "    converting B-spline curves to B-spline coefficients using least squares.\n",
    "    '''\n",
    "    # x_eval: (size, batch); y_eval: (size, batch); grid: (size, grid); k: scalar\n",
    "    # debug_structure(b=eval_spline(x_eval, grid, k), x=x_eval, grid=grid)\n",
    "    mat = jnp.permute_dims(jax.vmap(eval_spline, in_axes=(0, None, None))(x_eval, grid, k), (1, 0, 2))\n",
    "    y = jnp.expand_dims(y_eval.T, -1)\n",
    "    # debug_structure(mat=mat, x=x_eval, y=y)\n",
    "    # m n, m k -> n k\n",
    "    coef, _resid, _rank, _s = jax.vmap(jnp.linalg.lstsq, in_axes=0)(mat, y_eval.T)\n",
    "    return coef\n",
    "\n",
    "\n",
    "# x = np.random.randn(16, 5)\n",
    "# y = np.random.randn(16, 5)\n",
    "# grid = jnp.tile(jnp.linspace(-1, 1, 6)[None, :], (x.shape[1], 1))\n",
    "# # y_eval = eval_spline(x, grid, k=3, extend=True)\n",
    "# print(x.shape, y.shape, grid.shape)\n",
    "# curve2coef(x, y, grid, k=3).shape\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class KANLayerOutput:\n",
    "    y: Float[Array, \"out_dim\"]\n",
    "    postacts: Float[Array, \"out_dim in_dim\"]\n",
    "    postspline: Float[Array, \"out_dim in_dim\"]\n",
    "\n",
    "# @struct.dataclass\n",
    "class KANLayer(nn.Module):\n",
    "    in_dim: int\n",
    "    out_dim: int\n",
    "    n_grid: int = 5\n",
    "    order: int = 3\n",
    "    kernel_init: Callable = nn.initializers.normal(stddev=0.1)\n",
    "    resid_scale_trainable: bool = False\n",
    "    resid_scale_init: Callable = nn.initializers.ones\n",
    "    spline_scale_trainable: bool = False\n",
    "    spline_scale_init: Callable = nn.initializers.ones\n",
    "    base_act: Callable = nn.gelu\n",
    "    grid_range: tuple[float, float] = (-1, 1)\n",
    "\n",
    "    def setup(self):\n",
    "        self.size = self.in_dim * self.out_dim\n",
    "        self.grid = jnp.einsum('i,j->ij', jnp.ones(self.size), jnp.linspace(*self.grid_range, self.n_grid + 1))\n",
    "        def spline_init(*args, **kwargs):\n",
    "            noise = self.kernel_init(*args, **kwargs)          \n",
    "            # debug_structure(grid=self.grid, noise=noise)  \n",
    "            return curve2coef(self.grid.T, noise.T, self.grid, self.order)\n",
    "        \n",
    "        self.coef = self.param('coef', spline_init, (self.size, self.n_grid + 1))\n",
    "        if self.resid_scale_trainable:\n",
    "            self.resid_scale = self.param('resid_scale', self.resid_scale_init, (self.size,))\n",
    "        else:\n",
    "            self.resid_scale = 1\n",
    "\n",
    "        if self.spline_scale_trainable:\n",
    "            self.spline_scale = self.param('spline_scale', self.spline_scale_init, (self.size,))\n",
    "        else:\n",
    "            self.spline_scale = 1\n",
    "\n",
    "    def full_output(self, x: Float[Array, \"in_dim\"]) -> KANLayerOutput:\n",
    "        # splines: (out_dim in_dim)\n",
    "        x = jnp.tile(x[None, ...], (self.out_dim, 1)).reshape(-1)\n",
    "        y = coef2curve(x, self.grid, self.coef, self.order)\n",
    "        postspline = y.reshape(self.out_dim, self.in_dim)\n",
    "\n",
    "        y = self.resid_scale * self.base_act(x) + self.spline_scale * y\n",
    "        postacts = y.reshape(self.out_dim, self.in_dim)\n",
    "        \n",
    "        y = EinsOp('(out in) -> out', reduce='mean', symbol_values={'out': self.out_dim})(y)\n",
    "\n",
    "        return KANLayerOutput(y=y, postacts=postacts, postspline=postspline)\n",
    "    \n",
    "    def __call__(self, x: Float[Array, \"batch in_dim\"]) -> Float[Array, \"batch out_dim\"]:        \n",
    "        out = lambda x: self.full_output(x).y\n",
    "        return jax.vmap(out)(x)\n",
    "\n",
    "    \n",
    "\n",
    "# @struct.dataclass\n",
    "class KAN(nn.Module):\n",
    "    in_dim: int\n",
    "    out_dim: int\n",
    "    inner_dims: Sequence[int]\n",
    "\n",
    "    use_layernorm: bool = True\n",
    "    layer_templ: KANLayer = KANLayer(in_dim=1, out_dim=1)\n",
    "    final_act: Callable = Identity()\n",
    "\n",
    "    def setup(self):        \n",
    "        norms = []\n",
    "        layers = []\n",
    "        in_dim = self.in_dim\n",
    "        out_dims = tuple(self.inner_dims) + (self.out_dim,)        \n",
    "        for out_dim in out_dims:\n",
    "            layers.append(self.layer_templ.copy(in_dim=in_dim, out_dim=out_dim))\n",
    "            norms.append(nn.LayerNorm() if self.use_layernorm else Identity())\n",
    "            in_dim = out_dim\n",
    "\n",
    "        self.norms = norms\n",
    "        self.layers = layers\n",
    "        self.network = nn.Sequential(self.layers)\n",
    "\n",
    "    def full_outputs(self, x: Float[Array, \"in_dim\"]) -> tuple[Float[Array, \"out_dim\"], Sequence[KANLayerOutput]]:\n",
    "        outputs = []\n",
    "        curr_x = x\n",
    "        for layer, norm in zip(self.layers, self.norms):\n",
    "            curr_x = norm(curr_x)\n",
    "            outputs.append(layer.full_output(curr_x))\n",
    "            curr_x = outputs[-1].y\n",
    "        y = self.final_act(curr_x)\n",
    "        return y, outputs\n",
    "\n",
    "    def __call__(self, x: Float[Array, \"in_dim\"], training: bool = False) -> Float[Array, \"out_dim\"]:\n",
    "        out = lambda b: self.full_outputs(b)[0]\n",
    "        return jax.vmap(out)(x)\n",
    "\n",
    "\n",
    "in_dim = 4\n",
    "rng = jr.key(0)\n",
    "xtest = jr.normal(rng, (14, in_dim))\n",
    "\n",
    "kan = KAN(in_dim=in_dim, out_dim=6, inner_dims=[5])\n",
    "print(kan.tabulate(rng, xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[3m                                                                                              KAN Summary                                                                                               \u001b[0m\n",
      "                                                                                                                                                                                                        \n",
      " \u001b[1m \u001b[0m\u001b[1m          path\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m                           module\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m                       inputs\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m                                 outputs\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m           flops\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m       vjp_flops\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m                        params\u001b[0m\u001b[1m \u001b[0m \n",
      " ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
      "                                                 KAN                 training: False                             \u001b[2mfloat32\u001b[0m[512,1]              1.53G              3.09G                                   \n",
      "                                                                 x: \u001b[2mfloat32\u001b[0m[512,147]                                                                                                                    \n",
      "                                                                                                                                                                                                        \n",
      "         norms_0                           LayerNorm                    \u001b[2mfloat32\u001b[0m[147]                               \u001b[2mfloat32\u001b[0m[147]               1033               2957               bias: \u001b[2mfloat32\u001b[0m[147]  \n",
      "                                                                                                                                                                                   scale: \u001b[2mfloat32\u001b[0m[147]  \n",
      "                                                                                                                                                                                                        \n",
      "                                                                                                                                                                                          \u001b[1m294 \u001b[0m\u001b[1;2m(1.2 KB)\u001b[0m  \n",
      "                                                                                                                                                                                                        \n",
      "        layers_0              KANLayer \u001b[2m(full_output)\u001b[0m                    \u001b[2mfloat32\u001b[0m[147]                  postacts: \u001b[2mfloat32\u001b[0m[64,147]              3.64M              6.23M            coef: \u001b[2mfloat32\u001b[0m[9408,8]  \n",
      "                                                                                                    postspline: \u001b[2mfloat32\u001b[0m[64,147]                                                                         \n",
      "                                                                                                                 y: \u001b[2mfloat32\u001b[0m[64]                                                      \u001b[1m75,264 \u001b[0m\u001b[1;2m(301.1 KB)\u001b[0m  \n",
      "                                                                                                                                                                                                        \n",
      "         norms_1                           LayerNorm                     \u001b[2mfloat32\u001b[0m[64]                                \u001b[2mfloat32\u001b[0m[64]                452               1297                bias: \u001b[2mfloat32\u001b[0m[64]  \n",
      "                                                                                                                                                                                    scale: \u001b[2mfloat32\u001b[0m[64]  \n",
      "                                                                                                                                                                                                        \n",
      "                                                                                                                                                                                           \u001b[1m128 \u001b[0m\u001b[1;2m(512 B)\u001b[0m  \n",
      "                                                                                                                                                                                                        \n",
      "        layers_1              KANLayer \u001b[2m(full_output)\u001b[0m                     \u001b[2mfloat32\u001b[0m[64]                   postacts: \u001b[2mfloat32\u001b[0m[32,64]               793k              1.36M            coef: \u001b[2mfloat32\u001b[0m[2048,8]  \n",
      "                                                                                                     postspline: \u001b[2mfloat32\u001b[0m[32,64]                                                                         \n",
      "                                                                                                                 y: \u001b[2mfloat32\u001b[0m[32]                                                       \u001b[1m16,384 \u001b[0m\u001b[1;2m(65.5 KB)\u001b[0m  \n",
      "                                                                                                                                                                                                        \n",
      "         norms_2                           LayerNorm                     \u001b[2mfloat32\u001b[0m[32]                                \u001b[2mfloat32\u001b[0m[32]                228                657                bias: \u001b[2mfloat32\u001b[0m[32]  \n",
      "                                                                                                                                                                                    scale: \u001b[2mfloat32\u001b[0m[32]  \n",
      "                                                                                                                                                                                                        \n",
      "                                                                                                                                                                                            \u001b[1m64 \u001b[0m\u001b[1;2m(256 B)\u001b[0m  \n",
      "                                                                                                                                                                                                        \n",
      "        layers_2              KANLayer \u001b[2m(full_output)\u001b[0m                     \u001b[2mfloat32\u001b[0m[32]                    postacts: \u001b[2mfloat32\u001b[0m[1,32]              12411              21180              coef: \u001b[2mfloat32\u001b[0m[32,8]  \n",
      "                                                                                                      postspline: \u001b[2mfloat32\u001b[0m[1,32]                                                                         \n",
      "                                                                                                                  y: \u001b[2mfloat32\u001b[0m[1]                                                           \u001b[1m256 \u001b[0m\u001b[1;2m(1.0 KB)\u001b[0m  \n",
      "                                                                                                                                                                                                        \n",
      "       final_act                            Identity                      \u001b[2mfloat32\u001b[0m[1]                                 \u001b[2mfloat32\u001b[0m[1]                  0                  0                                   \n",
      " ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
      " \u001b[1m \u001b[0m\u001b[1m              \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m                                 \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m                             \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m                                        \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m                \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m           Total\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m             92,390 \u001b[0m\u001b[1;2m(369.6 KB)\u001b[0m\u001b[1m \u001b[0m \n",
      "                                                                                                                                                                                                        \n",
      "\u001b[1m                                                                                                                                                                                                        \u001b[0m\n",
      "\u001b[1m                                                                                  Total Parameters: 92,390 \u001b[0m\u001b[1;2m(369.6 KB)\u001b[0m\u001b[1m                                                                                   \u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[3m                                                                                           LazyInMLP Summary                                                                                            \u001b[0m\n",
      "                                                                                                                                                                                                        \n",
      " \u001b[1m \u001b[0m\u001b[1m                path\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m           module\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m                           inputs\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m                       outputs\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m             flops\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m         vjp_flops\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m                                    params\u001b[0m\u001b[1m \u001b[0m \n",
      " ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
      "                                 LazyInMLP                     training: False                   \u001b[2mfloat32\u001b[0m[512,1]                1.25G                3.73G                                               \n",
      "                                                           x: \u001b[2mfloat32\u001b[0m[512,147]                                                                                                                          \n",
      "                                                                                                                                                                                                        \n",
      "               Dense_0               Dense                    \u001b[2mfloat32\u001b[0m[512,147]                \u001b[2mfloat32\u001b[0m[512,1024]                 155M                 464M                          bias: \u001b[2mfloat32\u001b[0m[1024]  \n",
      "                                                                                                                                                                             kernel: \u001b[2mfloat32\u001b[0m[147,1024]  \n",
      "                                                                                                                                                                                                        \n",
      "                                                                                                                                                                                    \u001b[1m151,552 \u001b[0m\u001b[1;2m(606.2 KB)\u001b[0m  \n",
      "                                                                                                                                                                                                        \n",
      "             Dropout_0             Dropout                   \u001b[2mfloat32\u001b[0m[512,1024]                \u001b[2mfloat32\u001b[0m[512,1024]                    0                    0                                               \n",
      "                                                                                                                                                                                                        \n",
      "           LayerNorm_0           LayerNorm                   \u001b[2mfloat32\u001b[0m[512,1024]                \u001b[2mfloat32\u001b[0m[512,1024]                3.67M                10.5M                          bias: \u001b[2mfloat32\u001b[0m[1024]  \n",
      "                                                                                                                                                                                  scale: \u001b[2mfloat32\u001b[0m[1024]  \n",
      "                                                                                                                                                                                                        \n",
      "                                                                                                                                                                                        \u001b[1m2,048 \u001b[0m\u001b[1;2m(8.2 KB)\u001b[0m  \n",
      "                                                                                                                                                                                                        \n",
      "               Dense_1               Dense                   \u001b[2mfloat32\u001b[0m[512,1024]                \u001b[2mfloat32\u001b[0m[512,1024]                1.07G                3.22G                          bias: \u001b[2mfloat32\u001b[0m[1024]  \n",
      "                                                                                                                                                                            kernel: \u001b[2mfloat32\u001b[0m[1024,1024]  \n",
      "                                                                                                                                                                                                        \n",
      "                                                                                                                                                                                    \u001b[1m1,049,600 \u001b[0m\u001b[1;2m(4.2 MB)\u001b[0m  \n",
      "                                                                                                                                                                                                        \n",
      "             Dropout_1             Dropout                   \u001b[2mfloat32\u001b[0m[512,1024]                \u001b[2mfloat32\u001b[0m[512,1024]                    0                    0                                               \n",
      "                                                                                                                                                                                                        \n",
      "           LayerNorm_1           LayerNorm                   \u001b[2mfloat32\u001b[0m[512,1024]                \u001b[2mfloat32\u001b[0m[512,1024]                3.67M                10.5M                          bias: \u001b[2mfloat32\u001b[0m[1024]  \n",
      "                                                                                                                                                                                  scale: \u001b[2mfloat32\u001b[0m[1024]  \n",
      "                                                                                                                                                                                                        \n",
      "                                                                                                                                                                                        \u001b[1m2,048 \u001b[0m\u001b[1;2m(8.2 KB)\u001b[0m  \n",
      "                                                                                                                                                                                                        \n",
      "               Dense_2               Dense                   \u001b[2mfloat32\u001b[0m[512,1024]                   \u001b[2mfloat32\u001b[0m[512,1]                1.05M                3.15M                             bias: \u001b[2mfloat32\u001b[0m[1]  \n",
      "                                                                                                                                                                               kernel: \u001b[2mfloat32\u001b[0m[1024,1]  \n",
      "                                                                                                                                                                                                        \n",
      "                                                                                                                                                                                        \u001b[1m1,025 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m  \n",
      "                                                                                                                                                                                                        \n",
      "             final_act            Identity                      \u001b[2mfloat32\u001b[0m[512,1]                   \u001b[2mfloat32\u001b[0m[512,1]                    0                    0                                               \n",
      " ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
      " \u001b[1m \u001b[0m\u001b[1m                    \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m                 \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m                                 \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m                              \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m                  \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m             Total\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m                        1,206,273 \u001b[0m\u001b[1;2m(4.8 MB)\u001b[0m\u001b[1m \u001b[0m \n",
      "                                                                                                                                                                                                        \n",
      "\u001b[1m                                                                                                                                                                                                        \u001b[0m\n",
      "\u001b[1m                                                                                  Total Parameters: 1,206,273 \u001b[0m\u001b[1;2m(4.8 MB)\u001b[0m\u001b[1m                                                                                  \u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from jaxtyping import Bool\n",
    "\n",
    "dtype = jnp.float32\n",
    "\n",
    "@struct.dataclass\n",
    "class TrainBatch:\n",
    "    X: Float[Array, \"batch in_dim\"]\n",
    "    delta_e: Float[Array, \"batch\"]\n",
    "    mask: Bool[Array, \"batch\"]\n",
    "\n",
    "\n",
    "datasets = []\n",
    "for sub in df[is_valid], df[~is_valid]:\n",
    "    Xy = jnp.array(sub.values, dtype=dtype)\n",
    "    num_pad = -Xy.shape[0] % batch_size\n",
    "    mask = jnp.concat([jnp.ones(Xy.shape[0]), jnp.zeros(num_pad)]).astype(jnp.bool)\n",
    "    Xy = jnp.concat([Xy, Xy[:num_pad]])\n",
    "    datasets.append((Xy, mask))\n",
    "\n",
    "datasets = {'train': datasets[1], 'valid': datasets[0]}\n",
    "\n",
    "steps_in_epoch = Xy.shape[0] // batch_size\n",
    "\n",
    "def data_loader(split='train', infinite=False):\n",
    "    data, mas = datasets[split]\n",
    "    inds = np.arange(data.shape[0])\n",
    "\n",
    "    perm = np.random.permutation(inds)\n",
    "\n",
    "    data = jnp.array(data[perm])\n",
    "    mas = jnp.array(mas[perm])\n",
    "        \n",
    "    first_time = True\n",
    "    while first_time or infinite:\n",
    "        first_time = False\n",
    "        \n",
    "        for i in range(0, data.shape[0], batch_size):\n",
    "            yield TrainBatch(X=data[i:i+batch_size, :-1], delta_e=data[i:i+batch_size, -1] / 4, mask=mas[i:i+batch_size])\n",
    "\n",
    "sample_batch = next(data_loader())\n",
    "# debug_structure(sample_batch)\n",
    "\n",
    "\n",
    "from avid.layers import LazyInMLP\n",
    "\n",
    "sample_batch = next(data_loader())\n",
    "kan = KAN(in_dim=sample_batch.X.shape[-1], out_dim=1, inner_dims=[64, 32], use_layernorm=True)\n",
    "mlp = LazyInMLP(out_dim=1, inner_dims=[1024, 1024])\n",
    "\n",
    "kwargs = {'console_kwargs': {'width': 200}, 'compute_flops': True, 'compute_vjp_flops': True}\n",
    "flax_summary(kan, x=sample_batch.X, training=False, **kwargs)\n",
    "flax_summary(mlp, x=sample_batch.X, training=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LazyInMLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 0.009\tValid: 0.012: 100%|██████████| 250/250 [04:24<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KAN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 0.007\tValid: 0.010:  80%|████████  | 200/250 [04:11<01:01,  1.23s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import optax\n",
    "from ml_collections import ConfigDict\n",
    "from flax.training import train_state\n",
    "\n",
    "start_frac = 0.1\n",
    "end_frac = 0.3\n",
    "base_lr = 5e-3\n",
    "warmup = 10\n",
    "n_epochs = 250\n",
    "warmup_steps = steps_in_epoch * min(warmup, n_epochs // 4)\n",
    "sched = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=start_frac * base_lr,\n",
    "    peak_value=base_lr,\n",
    "    warmup_steps=warmup_steps,\n",
    "    decay_steps=steps_in_epoch * n_epochs,\n",
    "    end_value=end_frac*base_lr\n",
    ")\n",
    "\n",
    "def create_train_state(model, rng):\n",
    "    params = model.init(rng, sample_batch.X, training=True)['params']\n",
    "    tx = optax.adamw(sched)\n",
    "    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "steps_per_log = steps_in_epoch\n",
    "\n",
    "@ft.partial(jax.jit, static_argnames='training')\n",
    "def apply_model(state, batch: TrainBatch, training: bool):\n",
    "    def loss_fn(params):\n",
    "        yhat = state.apply_fn({'params': params}, batch.X, training=training)\n",
    "        err = jnp.abs(jnp.squeeze(yhat) - batch.delta_e) * batch.mask\n",
    "        return jnp.mean(err)\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grad = grad_fn(state.params)\n",
    "    return grad, loss\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module):\n",
    "    state = create_train_state(model, jr.key(np.random.randint(0, 1000)))\n",
    "    print(model.__class__.__name__)\n",
    "    epochs = []\n",
    "\n",
    "    with tqdm(np.arange(n_epochs)) as bar:\n",
    "        for epoch_i in bar:\n",
    "            losses = []\n",
    "            for batch in data_loader():\n",
    "                grad, loss = apply_model(state, batch, training=True) \n",
    "                losses.append(loss)\n",
    "                state = state.apply_gradients(grads=grad)\n",
    "\n",
    "            train_loss = np.mean(losses)\n",
    "\n",
    "            losses = []\n",
    "            for batch in data_loader(split='valid'):\n",
    "                grad, loss = apply_model(state, batch, training=False)            \n",
    "                losses.append(loss)\n",
    "                state = state.apply_gradients(grads=grad)\n",
    "\n",
    "            valid_loss = np.mean(losses)\n",
    "            epochs.append({'train': train_loss, 'valid': valid_loss})\n",
    "            bar.set_description(f'Train: {train_loss:.03f}\\tValid: {valid_loss:.03f}')\n",
    "\n",
    "    return state, epochs\n",
    "\n",
    "mlp_state, mlp_epochs = train_model(mlp)\n",
    "kan_state, kan_epochs = train_model(kan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = []\n",
    "for name, epochs in zip(('kan', 'mlp'), (kan_epochs, mlp_epochs)):\n",
    "    epochs = pd.DataFrame(epochs)\n",
    "    epochs['model'] = name    \n",
    "    hist.append(epochs.reset_index().rename(columns={'index': 'epoch'}))\n",
    "\n",
    "hist = pd.concat(hist).reset_index(drop=True)\n",
    "hist = hist.melt(var_name='dataset', value_name='loss', id_vars=['epoch', 'model'])\n",
    "hist\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "sns.lineplot(hist, x='epoch', y='loss', hue=hist[['model', 'dataset']].apply(' '.join, axis=1))\n",
    "plt.ylim(0, hist.query('epoch > 50')['loss'].max())\n",
    "rp.line_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from avid.utils import debug_stat\n",
    "\n",
    "coefs = kan_state.params['layers_0']['coef'].reshape(147, -1, 10)\n",
    "plt.plot(coefs[-1, :6, :].T);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some things to note from my first efforts training these KANs:\n",
    "- I get better results adding LayerNorms in between the layers like you would in an MLP.\n",
    "- I don't train the residual or spline scale factors after doing so, which reduces parameter count\n",
    "  significantly and seems to have negligible impact on accuracy.\n",
    "- I haven't implemented the learnable grid that the pykan repo has: it doesn't seem super important\n",
    "  after normalization.\n",
    "- The splines have a lot of hyperparameters to worry about, and I have to really sit down and write\n",
    "  an accelerated version of B-splines so this runs faster. There are lots of other ways to\n",
    "  parameterize flexible, smooth functions, maybe one of those would be better.\n",
    "  \n",
    "I'm quite impressed given that there's a lot of low-hanging fruit and it's already competitive with\n",
    "MLPs, which have tons of work behind them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
